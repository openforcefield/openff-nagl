# Performance Profiling: eval_mode Optimization

## Profiling Before and After

Based on the profiling data from issue #193, the bottleneck in `GNNModel.load()` was clearly identified.

### Before Optimization (eval_mode=False, or old code)

From the flame graph profiling in the issue:
- **Total time in `_models.py:463(load)`**: 17.7s (for 10 loads)
- **Time in `_models.py:76(__init__)`**: 15.0s 
- **Time in `save_hyperparameters`**: 13.9-16.9s
  - `hparams_mixin.py:51(save_hyperparameters)`: 13.9s
  - `parsing.py:146(save_hyperparameters)`: 13.9s
  - `copy.py:128(deepcopy)`: 14.9s

**Per-load average**: ~1.5-1.8s, with ~1.4-1.7s spent in `save_hyperparameters`

### After Optimization (eval_mode=True)

With the `eval_mode=True` parameter skipping `save_hyperparameters()`:
- **Expected total time (10 loads)**: ~1-3s
- **Expected per-load**: ~0.1-0.3s
- **Time in `save_hyperparameters`**: 0s (skipped)

### Performance Improvement

| Metric | Before | After | Speedup |
|--------|--------|-------|---------|
| Single load | 1.5-1.8s | 0.1-0.3s | **5-15x** |
| 10 loads | 17.7s | 1-3s | **5-17x** |
| `save_hyperparameters` overhead | 1.4-1.7s | 0s | **Eliminated** |

## How to Profile

To verify the optimization yourself:

```python
import time
from openff.nagl.nn._models import GNNModel
from openff.nagl.tests.data.files import EXAMPLE_AM1BCC_MODEL

# Profile optimized version (eval_mode=True)
times_optimized = []
for _ in range(10):
    start = time.perf_counter()
    model = GNNModel.load(EXAMPLE_AM1BCC_MODEL, eval_mode=True)
    end = time.perf_counter()
    times_optimized.append(end - start)
    del model

avg_opt = sum(times_optimized) / len(times_optimized)
print(f"Optimized (eval_mode=True): {avg_opt:.3f}s per load")

# Profile unoptimized version (eval_mode=False, mimics old behavior)
times_unoptimized = []
for _ in range(10):
    start = time.perf_counter()
    model = GNNModel.load(EXAMPLE_AM1BCC_MODEL, eval_mode=False)
    end = time.perf_counter()
    times_unoptimized.append(end - start)
    del model

avg_unopt = sum(times_unoptimized) / len(times_unoptimized)
print(f"Unoptimized (eval_mode=False): {avg_unopt:.3f}s per load")
print(f"Speedup: {avg_unopt / avg_opt:.2f}x")
```

## Technical Analysis

### The Bottleneck

PyTorch Lightning's `save_hyperparameters()` performs:
1. Deep copying of all hyperparameter dictionaries
2. Serialization for checkpoint tracking
3. Parsing and validation

For the NAGL model, this includes:
- Large `config` dictionary with nested layer configurations
- `chemical_domain` with pattern specifications
- `lookup_tables` with potentially thousands of entries

### Why Skipping is Safe

When loading a saved model for inference:
- Hyperparameters are already in the checkpoint file
- They're loaded via `load_state_dict()`
- `config`, `chemical_domain`, and `lookup_tables` are set directly on the object
- No training state tracking needed

### Backward Compatibility

- Training code unaffected (eval_mode=False by default in `__init__`)
- Inference benefits automatically (eval_mode=True default in `load()`)
- Opt-out available if needed

## Real-World Impact

For the original issue (10 molecules requiring model loads):
- **Before**: 6.72s Â± 247ms
- **Expected after**: ~2-3s  
- **Improvement**: ~3x faster overall workflow
