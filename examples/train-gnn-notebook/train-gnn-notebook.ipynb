{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480cf57f-a1ec-4c18-84a6-40af4268511e",
   "metadata": {},
   "source": [
    "# Training a Graph Neural Network with NAGL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed15cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook will go through the process of training a new Graph Neural Network (GNN) on a small dataset of alkanes, and demonstrate inference with the resulting model. On the way, we'll put together a tiny test dataset, and talk a bit about the architecture of the GNN we're training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbe5ab",
   "metadata": {},
   "source": [
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from openff.toolkit import Molecule\n",
    "from openff.units import unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804facd",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ef898-5084-43b8-86a6-315b4d095f67",
   "metadata": {},
   "source": [
    "First, we need a neural network to train! The neural network is called a \"model\", because once trained it amounts to a statistical model of some aspect of reality - in this case, a mapping from a molecule to a list of partial charges.\n",
    "\n",
    "NAGL's models start by defining a format to describe a molecule. General purpose formats that work well for humans, like SMILES strings or Kekule structures, tend not to work for AI. Humans have a huge amount of background knowledge; we can read letters, we can interpret visual images, and those of us that can read SMILES strings and Kekule structures know a lot about chemistry too. An untrained neural network doesn't know any of this, and a neural network big enough to learn it would take forever to train and might find it easier to just memorize the partial charges of the molecules in the training set.\n",
    "\n",
    "Instead, we want to use our human background knowledge to give the neural network as much relevant information as we can, so it can learn to generalize from the training set rather than memorize it. This is often the great challenge in training neural networks: Giving the network just the right information and abilities to glean a relationship from the training data. Too much and it can invent relationships that don't exist in reality but allow it to reproduce the training set slightly better; too little and it can't learn anything at all. This is also why it's important to separate the test, training and validation datasets!\n",
    "\n",
    "In a graph neural network, we describe a molecule as a *graph* of atoms. In mathematics, a graph is a collection of *nodes* that are connected by *edges*. Lots of everyday systems and objects are easy to describe as graphs: in public transit maps, nodes are stops and edges are routes; in geometric shapes, nodes are vertices and edges are, well, edges; on Twitter, nodes are people and edges are follows (or tweets or likes or all of the above!). In a molecular graph, nodes are atoms and edges are bonds. This allows us to apply computational science techniques developed for graphs to molecules. More on that later!\n",
    "\n",
    "To describe a graph, we first need to describe its nodes and edges. Then we can do some magic to turn it into a graph. The format we use to describe nodes and edges is a list of numbers that each describe a particular *feature* of the thing, so the construction of this format is called *featurization*. This lets us choose exactly what information the network has access to, and it lets us use our chemical background knowledge to provide the network with theoretical information that would help it make its decision, but would be too complicated to learn in training. In this example, this includes features like connectivity and ring size, even though that information is redundant with the graph itself; it turns out that having this information \"close to hand\" helps the neural network out!\n",
    "\n",
    "In NAGL, we construct a featurization for atoms by choosing a list of features from those in the [`features.atoms`] module, and for bonds by choosing a list of features from the [`features.bonds`] module. Once it has the featurization, NAGL can apply it to a molecule automatically. A lot of these features are useless in a dataset of acyclic alkanes, but we include them to demonstrate the sorts of features that are available:\n",
    "\n",
    "[`features.atoms`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.features.atoms.html\n",
    "[`features.bonds`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.features.bonds.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada8176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.features import atoms\n",
    "\n",
    "atom_features = (\n",
    "    atoms.AtomicElement(categories=[\"C\", \"H\"]), # Is the atom Carbon or Hydrogen?\n",
    "    atoms.AtomConnectivity(), # Is the atom bonded to 1, 2, 3, or 4 other atoms?\n",
    "    atoms.AtomAverageFormalCharge(), # What is the atom's mean formal charge over the molecule's tautomers?\n",
    "    atoms.AtomHybridization(), # What is the hybridization of the atom?\n",
    "    atoms.AtomInRingOfSize(ring_size=3), # Is the atom in a 3-membered ring?\n",
    "    atoms.AtomInRingOfSize(ring_size=4), # Is the atom in a 4-membered ring?\n",
    "    atoms.AtomInRingOfSize(ring_size=5), # Is the atom in a 5-membered ring?\n",
    "    atoms.AtomInRingOfSize(ring_size=6), # Is the atom in a 6-membered ring?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9dbff-bbc8-475c-a089-1b6b70f2a2e3",
   "metadata": {},
   "source": [
    "In addition to the description of the molecule, we also need to specify the architecture of the GNN. NAGL's GNNs consist of two modules, each of which is a neural network: Convolution, which incorporates information about its surroundings into the representation of each atom; and Readout, which computes some chemical property or properties from the convolved representation of an atom. Once the Readout module makes its prediction, a final post-processing layer can be applied to inject some human chemical knowledge on the output end; in the case of partial charges, the readout neural network predicts hardness and electronegativity, and charges are [computed analytically](https://doi.org/10.1021/ci034148o) from them.\n",
    "\n",
    "The GNN model should be configured via a `ModelConfig` object, with convolution and readout layers individually specified. This can be done in Python as specified below, although typically we expect to read from a YAML or JSON file (e.g. `ModelConfig.from_yaml_file` or `ModelConfig.parse_file`).\n",
    "\n",
    "In this example we use the [GraphSAGE] convolution layer architecture (no relation to the Sage force field!) with 3 hidden layers, each with 128 features. Note that these hidden layers include the output layer of the module, which is \"hidden\" by the next module, but do not include the input layer, which is specified by the `atom_features` and `bond_features` arguments. We first provide the model with the atom features we selected above. (We do not pass `bond_features` in the example below as GraphSAGE only makes use of atom features.) Then, we configure the convolution module by specifying its overall architecture and size.\n",
    "\n",
    "[GraphSAGE]: https://docs.dgl.ai/en/latest/generated/dgl.nn.tensorflow.conv.SAGEConv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f675f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.config.model import (\n",
    "    ConvolutionLayer,\n",
    "    ConvolutionModule,\n",
    ")\n",
    "from openff.nagl import GNNModel\n",
    "from openff.nagl.nn.gcn import SAGEConvStack\n",
    "from torch.nn import ReLU\n",
    "from openff.nagl.nn.postprocess import ComputePartialCharges\n",
    "\n",
    "single_convolution_layer = ConvolutionLayer(\n",
    "    hidden_feature_size=128,  # 128 features per hidden convolution layer\n",
    "    aggregator_type=\"mean\",  # aggregate atom representations with mean\n",
    "    activation_function=\"ReLU\", # max(0, x) activation function for layer\n",
    "    dropout=0.0, # no dropout\n",
    ")\n",
    "\n",
    "convolution_module = ConvolutionModule(\n",
    "    architecture=\"SAGEConv\", # GraphSAGE GCN\n",
    "    layers=[single_convolution_layer] * 3, # 3 hidden convolution layers        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36064769-5355-4493-9bfa-690a30d6a066",
   "metadata": {},
   "source": [
    "We then specify the readout module. We specify the number of dense layers between the pooling layer (whose size is set by the convolution module) and the optional post-processing layer, the number of features in each of these layers, and the activation function. Finally, we specify the `ComputePartialCharges` post-processing layer, which also adds a final layer with the appropriate number of features to the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce164b4-d150-4d8f-920d-d778f770c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.config.model import (\n",
    "    ForwardLayer,\n",
    "    ReadoutModule,\n",
    ")\n",
    "\n",
    "single_readout_layer = ForwardLayer(\n",
    "    hidden_feature_size=128,  # 128 features per hidden convolution layer\n",
    "    activation_function=\"ReLU\", # max(0, x) activation function for layer\n",
    "    dropout=0.0, # no dropout\n",
    ")\n",
    "\n",
    "readout_module = ReadoutModule(\n",
    "    pooling=\"atoms\",\n",
    "    layers=[single_readout_layer] * 4, # 4 internal readout layers\n",
    "    postprocess=\"compute_partial_charges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881c65c-5c8b-4c8b-b713-127b841f12bc",
   "metadata": {},
   "source": [
    "Now we can put them together in a full `ModelConfig`. This can be passed to create a `GNNModel`. A model can have multiple readouts that derive different properties from the convolution representation, so each readout module is specified in a dictionary with a label.\n",
    "\n",
    "Here, the [`GNNModel`] class represents all the hyperparameters for a model, but after we train it the same object will store weights as well.\n",
    "\n",
    "[`GNNModel`]: https://docs.openforcefield.org/projects/nagl/en/latest/api/generated/openff.nagl.GNNModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa53e2fb-f37f-4054-be1d-a8a471bd6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.config.model import ModelConfig\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    version=\"0.1\",\n",
    "    atom_features=atom_features,\n",
    "    bond_features=[],\n",
    "    convolution=convolution_module,\n",
    "    readouts={\n",
    "        \"predicted-am1bcc-charges\": readout_module\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de99f0d",
   "metadata": {},
   "source": [
    "## Put together our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbfb4c",
   "metadata": {},
   "source": [
    "When it comes down to it, a neural network infers a function from data and then interpolates that function's value for inputs that are outside the dataset. To get a good interpolation, we need both a robust functional form (the model), and a rich set of data that spans the many-dimensional space we'd like to interpolate over. To evaluate the quality of the interpolation, we need even more data to test the trained model on - data that wasn't used to train the model.\n",
    "\n",
    "Usually this takes thousands of data points, but for this demonstration we'll just use a dozen or so. By confining ourselves to short alkanes, we can get away with a small dataset.\n",
    "\n",
    "Traditionally, and in NAGL, data is split up into three categories to minimise overfitting:\n",
    "\n",
    "- **training**: Data the model is trained against\n",
    "\n",
    "- **validation**: Data used to validate the model as it is trained\n",
    "\n",
    "- **tests**: Data used to test that the final model is good\n",
    "\n",
    "In this example, we'll use a collection of ten molecules for training. This dataset is in the `labelled_alkanes` directory distributed with this notebook. We'll also build a test/validation dataset of 3 molecules by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9982e",
   "metadata": {},
   "source": [
    "We can use the [`LabelledDataset`] class to take a look at what's inside the `labelled_alkanes` directory (or use `pyarrow` directly). Later, we'll use it to store our custom test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c17298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_smiles</th>\n",
       "      <th>am1_charges</th>\n",
       "      <th>am1bcc_charges</th>\n",
       "      <th>custom_charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[H:2][C:1]([H:3])([H:4])[H:5]</td>\n",
       "      <td>[-0.2658799886703491, 0.06646999716758728, 0.0...</td>\n",
       "      <td>[-0.10868000239133835, 0.027170000597834587, 0...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H:3][C:1]([H:4])([H:5])[C:2]([H:6])([H:7])[H:8]</td>\n",
       "      <td>[-0.21174000017344952, -0.21174000017344952, 0...</td>\n",
       "      <td>[-0.09384000208228827, -0.09384000208228827, 0...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[H:4][C:1]([H:5])([H:6])[C:2]([H:7])([H:8])[C:...</td>\n",
       "      <td>[-0.21018000082536178, -0.15999999777837234, -...</td>\n",
       "      <td>[-0.09227999977090141, -0.08139999888160011, -...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[H:5][C:1]([H:6])([H:7])[C:2]([H:8])([H:9])[C:...</td>\n",
       "      <td>[-0.21003000438213348, -0.15905000269412994, -...</td>\n",
       "      <td>[-0.09212999844125339, -0.08044999891093799, -...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[H:5][C:1]([H:6])([H:7])[C:2]([H:8])([C:3]([H:...</td>\n",
       "      <td>[-0.20747000138674462, -0.10981000374470438, -...</td>\n",
       "      <td>[-0.08957000076770782, -0.07050999999046326, -...</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[H:6][C:1]([H:7])([H:8])[C:2]([H:9])([H:10])[C...</td>\n",
       "      <td>[-0.21004000306129456, -0.15812000632286072, -...</td>\n",
       "      <td>[-0.09213999658823013, -0.07952000200748444, -...</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[H:15][C:5]([H:16])([H:17])[C:4]([H:13])([H:14...</td>\n",
       "      <td>[-0.20766000405830495, -0.10704000250381582, -...</td>\n",
       "      <td>[-0.0897599982426447, -0.06774000100353185, -0...</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[H:7][C:1]([H:8])([H:9])[C:2]([H:10])([H:11])[...</td>\n",
       "      <td>[-0.21021999344229697, -0.15823000594973563, -...</td>\n",
       "      <td>[-0.0923200011253357, -0.0796300008893013, -0....</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[H:18][C:6]([H:19])([H:20])[C:5]([H:16])([H:17...</td>\n",
       "      <td>[-0.208649992197752, -0.1059999980032444, -0.2...</td>\n",
       "      <td>[-0.09075000137090683, -0.06669999659061432, -...</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[H:13][C:4]([H:14])([H:15])[C:3]([H:11])([H:12...</td>\n",
       "      <td>[-0.2068299949169159, -0.10380999743938446, -0...</td>\n",
       "      <td>[-0.08893000297248363, -0.06451000235974788, -...</td>\n",
       "      <td>[9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       mapped_smiles  \\\n",
       "0                      [H:2][C:1]([H:3])([H:4])[H:5]   \n",
       "1   [H:3][C:1]([H:4])([H:5])[C:2]([H:6])([H:7])[H:8]   \n",
       "2  [H:4][C:1]([H:5])([H:6])[C:2]([H:7])([H:8])[C:...   \n",
       "3  [H:5][C:1]([H:6])([H:7])[C:2]([H:8])([H:9])[C:...   \n",
       "4  [H:5][C:1]([H:6])([H:7])[C:2]([H:8])([C:3]([H:...   \n",
       "5  [H:6][C:1]([H:7])([H:8])[C:2]([H:9])([H:10])[C...   \n",
       "6  [H:15][C:5]([H:16])([H:17])[C:4]([H:13])([H:14...   \n",
       "7  [H:7][C:1]([H:8])([H:9])[C:2]([H:10])([H:11])[...   \n",
       "8  [H:18][C:6]([H:19])([H:20])[C:5]([H:16])([H:17...   \n",
       "9  [H:13][C:4]([H:14])([H:15])[C:3]([H:11])([H:12...   \n",
       "\n",
       "                                         am1_charges  \\\n",
       "0  [-0.2658799886703491, 0.06646999716758728, 0.0...   \n",
       "1  [-0.21174000017344952, -0.21174000017344952, 0...   \n",
       "2  [-0.21018000082536178, -0.15999999777837234, -...   \n",
       "3  [-0.21003000438213348, -0.15905000269412994, -...   \n",
       "4  [-0.20747000138674462, -0.10981000374470438, -...   \n",
       "5  [-0.21004000306129456, -0.15812000632286072, -...   \n",
       "6  [-0.20766000405830495, -0.10704000250381582, -...   \n",
       "7  [-0.21021999344229697, -0.15823000594973563, -...   \n",
       "8  [-0.208649992197752, -0.1059999980032444, -0.2...   \n",
       "9  [-0.2068299949169159, -0.10380999743938446, -0...   \n",
       "\n",
       "                                      am1bcc_charges custom_charges  \n",
       "0  [-0.10868000239133835, 0.027170000597834587, 0...            [0]  \n",
       "1  [-0.09384000208228827, -0.09384000208228827, 0...            [1]  \n",
       "2  [-0.09227999977090141, -0.08139999888160011, -...            [2]  \n",
       "3  [-0.09212999844125339, -0.08044999891093799, -...            [3]  \n",
       "4  [-0.08957000076770782, -0.07050999999046326, -...            [4]  \n",
       "5  [-0.09213999658823013, -0.07952000200748444, -...            [5]  \n",
       "6  [-0.0897599982426447, -0.06774000100353185, -0...            [6]  \n",
       "7  [-0.0923200011253357, -0.0796300008893013, -0....            [7]  \n",
       "8  [-0.09075000137090683, -0.06669999659061432, -...            [8]  \n",
       "9  [-0.08893000297248363, -0.06451000235974788, -...            [9]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openff.nagl.label.dataset import LabelledDataset\n",
    "\n",
    "dataset = LabelledDataset(\"labelled_alkanes\")\n",
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75907f1f",
   "metadata": {},
   "source": [
    "### Building a test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938cee0-ee07-4b6a-987e-215449149db7",
   "metadata": {},
   "source": [
    "To augment the provided training set, we'll quickly prepare a second dataset for testing and validation. This involves preparing a list of SMILES with partial charges and saving them into a PyArrow Dataset. The `LabelledDataset` class is used as a convenient wrapper. For more information on how to prepare a dataset, see the `prepare-dataset` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de9bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying labellers to batches: 0it [00:00, ?it/s]\n",
      "Assigning charges:   0%|                                                      | 0/3 [00:00<?, ?it/s]\u001b[AWarning: Cannot perform Hydrogen sampling with GPU-Omega: GPU-Omega disabled.\n",
      "\n",
      "Assigning charges:  33%|███████████████▎                              | 1/3 [00:00<00:00,  4.11it/s]\u001b[A\n",
      "Assigning charges: 100%|██████████████████████████████████████████████| 3/3 [00:00<00:00,  7.83it/s]\u001b[A\n",
      "Applying labellers to batches: 1it [00:00,  2.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mapped_smiles</th>\n",
       "      <th>am1bcc_charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[H:8][C:1]([H:9])([H:10])[C:2]([H:11])([H:12])...</td>\n",
       "      <td>[-0.09234000029771225, -0.07955999704806702, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H:7][C:1]([H:8])([H:9])[C:2]([H:10])([C:3]([H...</td>\n",
       "      <td>[-0.08924999833106995, -0.061900001019239426, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[H:6][C:1]([H:7])([H:8])[C:2]([C:3]([H:9])([H:...</td>\n",
       "      <td>[-0.08494000114938792, -0.05959999911925372, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       mapped_smiles  \\\n",
       "0  [H:8][C:1]([H:9])([H:10])[C:2]([H:11])([H:12])...   \n",
       "1  [H:7][C:1]([H:8])([H:9])[C:2]([H:10])([C:3]([H...   \n",
       "2  [H:6][C:1]([H:7])([H:8])[C:2]([C:3]([H:9])([H:...   \n",
       "\n",
       "                                      am1bcc_charges  \n",
       "0  [-0.09234000029771225, -0.07955999704806702, -...  \n",
       "1  [-0.08924999833106995, -0.061900001019239426, ...  \n",
       "2  [-0.08494000114938792, -0.05959999911925372, -...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openff.nagl.label.labels import LabelCharges\n",
    "\n",
    "# Choose the molecules to put in this dataset\n",
    "# Note that these molecules aren't in the training dataset!\n",
    "test_smiles = [\n",
    "    \"CCCCCCC\",\n",
    "    \"CC(C)C(C)C\",\n",
    "    \"CC(C)(C)C\",\n",
    "]\n",
    "\n",
    "test_dataset = LabelledDataset.from_smiles(\n",
    "    \"my_first_test_dataset\",  # path to save to\n",
    "    test_smiles,\n",
    "    mapped=False,\n",
    "    overwrite_existing=True,\n",
    ")\n",
    "\n",
    "am1bcc_labeller = LabelCharges(\n",
    "    charge_method=\"am1bcc\",\n",
    "    charge_column=\"am1bcc_charges\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "test_dataset.apply_labellers(\n",
    "    [am1bcc_labeller],\n",
    "    verbose=True,\n",
    ")\n",
    "test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26946d2",
   "metadata": {},
   "source": [
    "### Curating our data module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f2cb9-10ec-4df9-9019-3bc2c38d3fca",
   "metadata": {},
   "source": [
    "The [data module] is responsible for providing featurized data to the model as it is fitted. As with the model, the datasets we use should be specified by a config object. Ideally users should not interact with a `DGLMoleculeDataModule` directly. Fitting is done in parallel batches whose size can be tweaked to manage the available memory; our datasets are small enough that all fitting will be done at once.\n",
    "\n",
    "One of the strengths of using Arrow datasets is that we can choose which columns to load into memory as needed for training. That means that for the `DatasetConfig`, we need to specify the targets we are choosing to fit. A `Target` is what we used to construct the objective function and calculate loss.\n",
    "\n",
    "[data module]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.training.training.DGLMoleculeDataModule.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f67b4f8-72d2-455f-aa2e-1b3301786809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.config.data import DatasetConfig, DataConfig\n",
    "from openff.nagl.training.metrics import RMSEMetric\n",
    "from openff.nagl.training.loss import ReadoutTarget\n",
    "\n",
    "\n",
    "charge_rmse_target = ReadoutTarget(\n",
    "    metric=RMSEMetric(),  # use RMSE to calculate loss\n",
    "    target_label=\"am1bcc_charges\", # column to use from data as reference target\n",
    "    prediction_label=\"predicted-am1bcc-charges\", # readout value to compare to target\n",
    "    denominator=1.0, # denominator to normalise loss -- important for multi-target objectives\n",
    "    weight=1.0, # how much to weight the loss -- important for multi-target objectives\n",
    ")\n",
    "\n",
    "training_dataset_config = DatasetConfig(\n",
    "    sources=[\"labelled_alkanes\"],\n",
    "    targets=[charge_rmse_target],\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "test_dataset_config = validation_dataset_config = DatasetConfig(\n",
    "    sources=[\"my_first_test_dataset\"],\n",
    "    targets=[charge_rmse_target],\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "data_config = DataConfig(\n",
    "    training=training_dataset_config,\n",
    "    validation=validation_dataset_config,\n",
    "    test=test_dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e228a5",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e1846-fd1d-4c35-9653-d658218a9c41",
   "metadata": {},
   "source": [
    "We've prepared our model architecture and our training, validation and test data; now we just need to fit the model! To do this, we need to specify optimization settings with a `OptimizerConfig`, and then put everything together in a `TrainingConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7320ab2-8412-40a8-ac74-4346412e3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.config.optimizer import OptimizerConfig\n",
    "from openff.nagl.config.training import TrainingConfig\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=\"Adam\",\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    model=model_config,\n",
    "    data=data_config,\n",
    "    optimizer=optimizer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7897285f-6039-4c8c-aa2f-c685f814c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.training.training import TrainingGNNModel, DGLMoleculeDataModule\n",
    "\n",
    "training_model = TrainingGNNModel(training_config)\n",
    "data_module = DGLMoleculeDataModule(training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8195a2b-2f27-462b-b607-c8327a780291",
   "metadata": {},
   "source": [
    "To properly fit the model, we use the [`Trainer`] class from PyTorch Lightning. This allows us to configure how data and progress are stored and reported using callbacks. The [`fit()`] method trains and validates against the data module we provide it: \n",
    "\n",
    "[`Trainer`]: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html\n",
    "[`fit()`]: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe8f0fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 155.17it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 14.98it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 143.42it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 43.71it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 142.30it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 43.50it/s]\n",
      "2023-12-19 12:01:50.343911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 173.16it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 16.72it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 147.38it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 44.75it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 145.25it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 44.03it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | GNNModel | 136 K \n",
      "-----------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.546     Total estimated model params size (MB)\n",
      "/home/joshmitchell/Documents/openff/nagl/.soap/examples/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/joshmitchell/Documents/openff/nagl/.soap/examples/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/joshmitchell/Documents/openff/nagl/.soap/examples/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_epochs=200)\n",
    "\n",
    "trainer.progress_bar_callback.disable()\n",
    "trainer.checkpoint_callback.monitor = \"val/loss\"\n",
    "\n",
    "trainer.fit(\n",
    "    training_model,\n",
    "    datamodule=data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4de68",
   "metadata": {},
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43eeb50",
   "metadata": {},
   "source": [
    "We can use the `Trainer` object's [`test()`] method to evaluate the model against our test data.\n",
    "The output has two entries:\n",
    "- `test/am1bcc_charges/readout/rmse/1.0/1.0`: the charge RMSE loss\n",
    "- `test/loss`: the total loss\n",
    "\n",
    "The numbers are the same here as this is a single target objective.\n",
    "\n",
    "[`test()`]: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ffade64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 141.05it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 13.67it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 144.19it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 44.15it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 144.45it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 44.22it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 176.76it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 17.10it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 142.07it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 43.69it/s]\n",
      "Featurizing dataset: 0it [00:00, ?it/s]\n",
      "Featurizing batch: 100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 143.13it/s]\u001b[A\n",
      "Featurizing dataset: 1it [00:00, 43.97it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/joshmitchell/Documents/openff/nagl/.soap/examples/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">               Test metric                </span>┃<span style=\"font-weight: bold\">               DataLoader 0               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test/am1bcc_charges/readout/rmse/1.0/1.0 </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.0023444981779903173           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">                test/loss                 </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.0023444981779903173           </span>│\n",
       "└──────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m              Test metric               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m              DataLoader 0              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest/am1bcc_charges/readout/rmse/1.0/1.0\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.0023444981779903173          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m               test/loss                \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.0023444981779903173          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/am1bcc_charges/readout/rmse/1.0/1.0': 0.0023444981779903173,\n",
       "  'test/loss': 0.0023444981779903173}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(training_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae1cc7-8f9b-421d-9343-b03f806425ba",
   "metadata": {},
   "source": [
    "We can isolate the model itself from all the training requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffaaec52-8a1d-4157-b728-b6373bcde57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc2260",
   "metadata": {},
   "source": [
    "Octane isn't in any of our data, so the model hasn't seen it yet! We can predict it's partial charges with the [`compute_property()`] method:\n",
    "\n",
    "[`compute_property()`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.GNNModel.html#openff.nagl.GNNModel.compute_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87609fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08929691, -0.07779237, -0.07612573, -0.07613489, -0.07613489,\n",
       "       -0.07612573, -0.07779235, -0.08929689,  0.03182327,  0.03182327,\n",
       "        0.03182327,  0.0371701 ,  0.0371701 ,  0.03738499,  0.03738499,\n",
       "        0.03738499,  0.03738499,  0.03738499,  0.03738499,  0.03738499,\n",
       "        0.03738499,  0.0371701 ,  0.0371701 ,  0.03182329,  0.03182334,\n",
       "        0.03182334], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octane = Molecule.from_smiles(\"CCCCCCCC\")\n",
    "\n",
    "model.compute_property(octane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5831c8e",
   "metadata": {},
   "source": [
    "And we can compare that to the partial charges produced by the OpenFF Toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e12208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th>Magnitude</th><td style='text-align:left;'><pre>[-0.09216000225681525 -0.07999000039238197 -0.079319999481623<br> -0.07835999962228996 -0.07835999962228996 -0.079319999481623<br> -0.07999000039238197 -0.09207999792236549 0.03245000082712907<br> 0.03245000082712907 0.03245000082712907 0.03792999971371431<br> 0.03792999971371431 0.038880000893886275 0.038880000893886275<br> 0.03940999794464845 0.03940999794464845 0.03940999794464845<br> 0.03940999794464845 0.038880000893886275 0.038880000893886275<br> 0.03792999971371431 0.03792999971371431 0.03245000082712907<br> 0.03245000082712907 0.03245000082712907]</pre></td></tr><tr><th>Units</th><td style='text-align:left;'>elementary_charge</td></tr></tbody></table>"
      ],
      "text/latex": [
       "$\\begin{pmatrix}-0.09216000225681525 & -0.07999000039238197 & -0.079319999481623 & -0.07835999962228996 & -0.07835999962228996 & -0.079319999481623 & -0.07999000039238197 & -0.09207999792236549 & 0.03245000082712907 & 0.03245000082712907 & 0.03245000082712907 & 0.03792999971371431 & 0.03792999971371431 & 0.038880000893886275 & 0.038880000893886275 & 0.03940999794464845 & 0.03940999794464845 & 0.03940999794464845 & 0.03940999794464845 & 0.038880000893886275 & 0.038880000893886275 & 0.03792999971371431 & 0.03792999971371431 & 0.03245000082712907 & 0.03245000082712907 & 0.03245000082712907\\end{pmatrix}\\ \\mathrm{elementary\\_charge}$"
      ],
      "text/plain": [
       "array([-0.09216, -0.07999, -0.07932, -0.07836, -0.07836, -0.07932,\n",
       "       -0.07999, -0.09208,  0.03245,  0.03245,  0.03245,  0.03793,\n",
       "        0.03793,  0.03888,  0.03888,  0.03941,  0.03941,  0.03941,\n",
       "        0.03941,  0.03888,  0.03888,  0.03793,  0.03793,  0.03245,\n",
       "        0.03245,  0.03245]) <Unit('elementary_charge')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octane.assign_partial_charges(\"am1bcc\")\n",
    "octane.partial_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ef45d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th>Magnitude</th><td style='text-align:left;'><pre>[0.0028630950703070757 0.00219763156313163 0.00319427337784034<br> 0.002225109304373085 0.002225109304373085 0.00319427337784034<br> 0.002197653914873421 0.0027831056370185014 0.0006267308042599631<br> 0.0006267308042599631 0.0006267308042599631 0.0007598950312687827<br> 0.0007598950312687827 0.0014950065658642722 0.0014950065658642722<br> 0.0020250036166264487 0.0020250036166264487 0.0020250036166264487<br> 0.0020250036166264487 0.0014950065658642722 0.0014950065658642722<br> 0.0007598950312687827 0.0007598950312687827 0.0006267084525181724<br> 0.0006266562984539939 0.0006266637490345908]</pre></td></tr><tr><th>Units</th><td style='text-align:left;'>elementary_charge</td></tr></tbody></table>"
      ],
      "text/latex": [
       "$\\begin{pmatrix}0.0028630950703070757 & 0.00219763156313163 & 0.00319427337784034 & 0.002225109304373085 & 0.002225109304373085 & 0.00319427337784034 & 0.002197653914873421 & 0.0027831056370185014 & 0.0006267308042599631 & 0.0006267308042599631 & 0.0006267308042599631 & 0.0007598950312687827 & 0.0007598950312687827 & 0.0014950065658642722 & 0.0014950065658642722 & 0.0020250036166264487 & 0.0020250036166264487 & 0.0020250036166264487 & 0.0020250036166264487 & 0.0014950065658642722 & 0.0014950065658642722 & 0.0007598950312687827 & 0.0007598950312687827 & 0.0006267084525181724 & 0.0006266562984539939 & 0.0006266637490345908\\end{pmatrix}\\ \\mathrm{elementary\\_charge}$"
      ],
      "text/plain": [
       "array([0.0028631 , 0.00219763, 0.00319427, 0.00222511, 0.00222511,\n",
       "       0.00319427, 0.00219765, 0.00278311, 0.00062673, 0.00062673,\n",
       "       0.00062673, 0.0007599 , 0.0007599 , 0.00149501, 0.00149501,\n",
       "       0.002025  , 0.002025  , 0.002025  , 0.002025  , 0.00149501,\n",
       "       0.00149501, 0.0007599 , 0.0007599 , 0.00062671, 0.00062666,\n",
       "       0.00062666]) <Unit('elementary_charge')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.compute_property(octane, as_numpy=True) * unit.elementary_charge\n",
    "np.abs(prediction - octane.partial_charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a81b2f",
   "metadata": {},
   "source": [
    "All within 0.02 elementary charge units of true AM1BCC charges! Not too bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f055919",
   "metadata": {},
   "source": [
    "## Saving and loading our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6140",
   "metadata": {},
   "source": [
    "We can save the final model with the `model.save()` method. This'll let us store it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7020a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_alkane_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7dec08",
   "metadata": {},
   "source": [
    "When we want it again, we can use the `GNNModel.load()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6beb2faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08929691, -0.07779237, -0.07612573, -0.07613489, -0.07613489,\n",
       "       -0.07612573, -0.07779235, -0.08929689,  0.03182327,  0.03182327,\n",
       "        0.03182327,  0.0371701 ,  0.0371701 ,  0.03738499,  0.03738499,\n",
       "        0.03738499,  0.03738499,  0.03738499,  0.03738499,  0.03738499,\n",
       "        0.03738499,  0.0371701 ,  0.0371701 ,  0.03182329,  0.03182334,\n",
       "        0.03182334], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_from_disk = GNNModel.load(\"trained_alkane_model.pt\")\n",
    "model_from_disk.compute_property(octane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24399a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
