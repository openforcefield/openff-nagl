{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480cf57f-a1ec-4c18-84a6-40af4268511e",
   "metadata": {},
   "source": [
    "# Training a Graph Neural Network with NAGL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed15cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook will go through the process of training a new Graph Neural Network (GNN) on a small dataset of alkanes, and demonstrate inference with the resulting model. On the way, we'll put together a tiny test dataset, and talk a bit about the architecture of the GNN we're training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbe5ab",
   "metadata": {},
   "source": [
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from openff.toolkit import Molecule\n",
    "from openff.units import unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804facd",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ef898-5084-43b8-86a6-315b4d095f67",
   "metadata": {},
   "source": [
    "First, we need a neural network to train! The neural network is called a \"model\", because once trained it amounts to a statistical model of some aspect of reality - in this case, a mapping from a molecule to a list of partial charges.\n",
    "\n",
    "NAGL's models start by defining a format to describe a molecule. General purpose formats that work well for humans, like SMILES strings or Kekule structures, tend not to work for AI. Humans have a huge amount of background knowledge; we can read letters, we can interpret visual images, and those of us that can read SMILES strings and Kekule structures know a lot about chemistry too. An untrained neural network doesn't know any of this, and a neural network big enough to learn it would take forever to train and might find it easier to just memorize the partial charges of the molecules in the training set.\n",
    "\n",
    "Instead, we want to use our human background knowledge to give the neural network as much relevant information as we can, so it can learn to generalize from the training set rather than memorize it. This is often the great challenge in training neural networks: Giving the network just the right information and abilities to glean a relationship from the training data. Too much and it can invent relationships that don't exist in reality but allow it to reproduce the training set slightly better; too little and it can't learn anything at all. This is also why it's important to separate the test, training and validation datasets!\n",
    "\n",
    "In a graph neural network, we describe a molecule as a *graph* of atoms. In mathematics, a graph is a collection of *nodes* that are connected by *edges*. Lots of everyday systems and objects are easy to describe as graphs: in public transit maps, nodes are stops and edges are routes; in geometric shapes, nodes are vertices and edges are, well, edges; on Twitter, nodes are people and edges are follows (or tweets or likes or all of the above!). In a molecular graph, nodes are atoms and edges are bonds. This allows us to apply computational science techniques developed for graphs to molecules. More on that later!\n",
    "\n",
    "To describe a graph, we first need to describe its nodes and edges. Then we can do some magic to turn it into a graph. The format we use to describe nodes and edges is a list of numbers that each describe a particular *feature* of the thing, so the construction of this format is called *featurization*. This lets us choose exactly what information the network has access to, and it lets us use our chemical background knowledge to provide the network with theoretical information that would help it make its decision, but would be too complicated to learn in training. In this example, this includes features like connectivity and ring size, even though that information is redundant with the graph itself; it turns out that having this information \"close to hand\" helps the neural network out!\n",
    "\n",
    "In NAGL, we construct a featurization for atoms by choosing a list of features from those in the [`features.atoms`] module, and for bonds by choosing a list of features from the [`features.bonds`] module. Once it has the featurization, NAGL can apply it to a molecule automatically. A lot of these features are useless in a dataset of acyclic alkanes, but we include them to demonstrate the sorts of features that are available:\n",
    "\n",
    "[`features.atoms`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.features.atoms.html\n",
    "[`features.bonds`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.features.bonds.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada8176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.features import atoms, bonds\n",
    "\n",
    "atom_features = (\n",
    "    atoms.AtomicElement([\"C\", \"H\"]), # Is the atom Carbon or Hydrogen?\n",
    "    atoms.AtomConnectivity(), # Is the atom bonded to 1, 2, 3, or 4 other atoms?\n",
    "    atoms.AtomAverageFormalCharge(), # What is the atom's mean formal charge over the molecule's tautomers?\n",
    "    atoms.AtomHybridization(), # What is the hybridization of the atom?\n",
    "    atoms.AtomInRingOfSize(3), # Is the atom in a 3-membered ring?\n",
    "    atoms.AtomInRingOfSize(4), # Is the atom in a 4-membered ring?\n",
    "    atoms.AtomInRingOfSize(5), # Is the atom in a 5-membered ring?\n",
    "    atoms.AtomInRingOfSize(6), # Is the atom in a 6-membered ring?\n",
    ")\n",
    "\n",
    "bond_features = (\n",
    "    bonds.BondInRingOfSize(3), # Is the bond in a 3-membered ring?\n",
    "    bonds.BondInRingOfSize(4), # Is the bond in a 4-membered ring?\n",
    "    bonds.BondInRingOfSize(5), # Is the bond in a 5-membered ring?\n",
    "    bonds.BondInRingOfSize(6), # Is the bond in a 6-membered ring?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9dbff-bbc8-475c-a089-1b6b70f2a2e3",
   "metadata": {},
   "source": [
    "In addition to the description of the molecule, we also need to specify the architecture of the GNN. NAGL's GNNs consist of two modules, each of which is a neural network: Convolution, which incorporates information about its surroundings into the representation of each atom; and Readout, which computes some chemical property or properties from the convolved representation of an atom. Once the Readout module makes its prediction, a final post-processing layer can be applied to inject some human chemical knowledge on the output end; in the case of partial charges, the readout neural network predicts hardness and electronegativity, and charges are [computed analytically](https://doi.org/10.1021/ci034148o) from them.\n",
    "\n",
    "Both modules are configured as arguments to the `GNNModel` initialization method. Here, the [`GNNModel`] class represents all the hyperparameters for a model, but after we train it the same object will store weights as well.\n",
    "\n",
    "We first provide the model with the atom and bond features we selected above. Then, we configure the convolution module by specifying its overall architecture and size. In this example we use the [GraphSAGE] convolution layer architecture (no relation to the Sage force field!) with 3 hidden layers, each with 128 features. Note that these hidden layers include the output layer of the module, which is \"hidden\" by the next module, but do not include the input layer, which is specified by the `atom_features` and `bond_features` arguments.\n",
    "\n",
    "We then specify the readout module. We specify the number of layers between the pooling layer (whose size is set by the convolution module) and the optional post-processing layer, the number of features in each of these layers, and the activation function. Finally, we specify the `ComputePartialCharges` post-processing layer, which also adds a final layer with the appropriate number of features to the module.\n",
    "\n",
    "[`GNNModel`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.GNNModel.html\n",
    "[GraphSAGE]: https://docs.dgl.ai/en/latest/generated/dgl.nn.tensorflow.conv.SAGEConv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f675f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl import GNNModel\n",
    "from openff.nagl.nn.gcn import SAGEConvStack\n",
    "from torch.nn import ReLU\n",
    "from openff.nagl.nn.postprocess import ComputePartialCharges\n",
    "\n",
    "model = GNNModel(\n",
    "    atom_features=atom_features,\n",
    "    bond_features=bond_features,\n",
    "    convolution_architecture=SAGEConvStack, # GraphSAGE GCN\n",
    "    n_convolution_hidden_features=128, # 128 features per hidden convolution layer\n",
    "    n_convolution_layers=3, # 3 hidden convolution layers\n",
    "    n_readout_hidden_features=128, # 128 features per internal readout layer\n",
    "    n_readout_layers=4, # 4 internal readout layers\n",
    "    activation_function=ReLU, # max(0, x) activation function for readout layer\n",
    "    postprocess_layer=ComputePartialCharges, # Add a 2-feature output layer to readout and compute charge from it\n",
    "    readout_name=f\"am1bcc-charges\",\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de99f0d",
   "metadata": {},
   "source": [
    "## Put together our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbfb4c",
   "metadata": {},
   "source": [
    "When it comes down to it, a neural network infers a function from data and then interpolates that function's value for inputs that are outside the dataset. To get a good interpolation, we need both a robust functional form (the model), and a rich set of data that spans the many-dimensional space we'd like to interpolate over. To evaluate the quality of the interpolation, we need even more data to test the trained model on - data that wasn't used to train the model.\n",
    "\n",
    "Usually this takes thousands of data points, but for this demonstration we'll just use a dozen or so. By confining ourselves to short alkanes, we can get away with a small dataset.\n",
    "\n",
    "Traditionally, and in NAGL, data is split up into three categories to minimise overfitting:\n",
    "\n",
    "- **training**: Data the model is trained against\n",
    "\n",
    "- **validation**: Data used to validate the model as it is trained\n",
    "\n",
    "- **tests**: Data used to test that the final model is good\n",
    "\n",
    "In this example, we'll use a collection of ten molecules for training. This dataset is in the `alkanes.sqlite` file distributed with this notebook. We'll also build a test/validation dataset of 3 molecules by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9982e",
   "metadata": {},
   "source": [
    "We can use the [`MoleculeStore`] class to take a look at what's inside the `alkanes.sqlite` file. Later, we'll use it to store our custom test dataset:\n",
    "\n",
    "[`MoleculeStore`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.storage.MoleculeStore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c17298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCC(C)CC',\n",
       " 'CCCC(C)C',\n",
       " 'CCCCCC',\n",
       " 'CCCCC',\n",
       " 'CCC(C)C',\n",
       " 'CCCC',\n",
       " 'CC(C)C',\n",
       " 'CCC',\n",
       " 'CC',\n",
       " 'C']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openff.nagl.storage import MoleculeStore\n",
    "\n",
    "dataset = MoleculeStore(\"alkanes.sqlite\")\n",
    "training_molecules = [Molecule.from_smiles(smiles) for smiles in dataset.get_smiles()]\n",
    "\n",
    "[mol.to_smiles(explicit_hydrogens=False) for mol in training_molecules]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75907f1f",
   "metadata": {},
   "source": [
    "### Building a test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938cee0-ee07-4b6a-987e-215449149db7",
   "metadata": {},
   "source": [
    "To augment the provided training set, we'll quickly prepare a second dataset for testing and validation. This involves preparing a list of [`MoleculeRecord`] objects with partial charges and saving them into a SQLite database. For more information on how to prepare a dataset, see the `prepare-dataset` example:\n",
    "\n",
    "[`MoleculeRecord`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.storage.record.MoleculeRecord.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de9bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grouping records to store by InChI key: 100%|████| 3/3 [00:00<00:00, 276.34it/s]\n",
      "storing grouped records: 100%|███████████████████| 3/3 [00:00<00:00, 207.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from openff.nagl.storage.record import MoleculeRecord\n",
    "from openff.nagl.storage import MoleculeStore\n",
    "\n",
    "# Choose the molecules to put in this dataset\n",
    "# Note that these molecules aren't in the training dataset!\n",
    "test_smiles = [\n",
    "    \"CCCCCCC\",\n",
    "    \"CC(C)C(C)C\",\n",
    "    \"CC(C)(C)C\",\n",
    "]\n",
    "\n",
    "records = []\n",
    "for smiles in test_smiles:\n",
    "    # Create a Molecule object\n",
    "    molecule = Molecule.from_smiles(smiles, allow_undefined_stereo=True)\n",
    "    # Generate a conformer for charge assignment\n",
    "    # Note that a production dataset should include some method to produce\n",
    "    # conformation-independent charges\n",
    "    molecule.generate_conformers(n_conformers=1)\n",
    "    # Compute partial charges\n",
    "    molecule.assign_partial_charges(\"am1bcc\")\n",
    "    # Create a MoleculeRecord\n",
    "    record = MoleculeRecord.from_precomputed_openff(\n",
    "        molecule,\n",
    "        partial_charge_method=\"am1bcc\"\n",
    "    )\n",
    "    # Add the record to the list\n",
    "    records.append(record)\n",
    "    \n",
    "# Save the dataset\n",
    "test_set_path = Path(\"my_first_test_dataset.sqlite\")\n",
    "if test_set_path.exists():\n",
    "    test_set_path.unlink()\n",
    "MoleculeStore(test_set_path).store(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26946d2",
   "metadata": {},
   "source": [
    "### Curating our data module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcf7ed",
   "metadata": {},
   "source": [
    "The [data module] is responsible for providing featurized data to the model as it is fitted. It therefore needs the featurization scheme, as well as the paths to the training, validation and test data sets. Fitting is done in parallel batches whose size can be tweaked to manage the available memory; our datasets are small enough that all fitting will be done at once.\n",
    "\n",
    "[data module]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.nn.dataset.DGLMoleculeLightningDataModule.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed8818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.nagl.nn.dataset import DGLMoleculeLightningDataModule\n",
    "\n",
    "# Remove the output directory (in case we're re-running the notebook)\n",
    "if Path(\"./data/\").exists():\n",
    "    !rm -r data\n",
    "\n",
    "data_module = DGLMoleculeLightningDataModule(\n",
    "    atom_features=atom_features,\n",
    "    bond_features=bond_features,\n",
    "    partial_charge_method=\"am1bcc\",\n",
    "    training_set_paths=[Path(\"alkanes.sqlite\")],\n",
    "    validation_set_paths=[test_set_path],\n",
    "    test_set_paths=[test_set_path],\n",
    "    training_batch_size=1000,\n",
    "    validation_batch_size=1000,\n",
    "    test_batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e228a5",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d010d1",
   "metadata": {},
   "source": [
    "We've prepared our model architecture and our training, validation and test data; now we just need to fit the model! To do this, we use the [`Trainer`] class from PyTorch Lightning. This allows us to configure how data and progress are stored and reported using callbacks. The [`fit()`] method trains and validates against the data module we provide it: \n",
    "\n",
    "[`Trainer`]: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html\n",
    "[`fit()`]: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe8f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "featurizing molecules: 100%|████████████████████| 10/10 [00:00<00:00, 56.14it/s]\n",
      "featurizing molecules: 100%|██████████████████████| 3/3 [00:00<00:00, 45.74it/s]\n",
      "\n",
      "  | Name               | Type              | Params\n",
      "---------------------------------------------------------\n",
      "0 | convolution_module | ConvolutionModule | 70.3 K\n",
      "1 | readout_modules    | ModuleDict        | 66.3 K\n",
      "---------------------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.546     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_epochs=200)\n",
    "\n",
    "trainer.progress_bar_callback.disable()\n",
    "trainer.checkpoint_callback.monitor = \"val_loss\"\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4de68",
   "metadata": {},
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43eeb50",
   "metadata": {},
   "source": [
    "We can use the `Trainer` object's [`test()`] method to evaluate the model against our test data:\n",
    "\n",
    "[`test()`]: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html#pytorch_lightning.trainer.trainer.Trainer.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffade64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.002892725635319948    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.002892725635319948   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.002892725635319948}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc2260",
   "metadata": {},
   "source": [
    "Octane isn't in any of our data, so the model hasn't seen it yet! We can predict it's partial charges with the [`compute_property()`] method:\n",
    "\n",
    "[`compute_property()`]: https://docs.openforcefield.org/projects/nagl/en/stable/api/generated/openff.nagl.GNNModel.html#openff.nagl.GNNModel.compute_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87609fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0938],\n",
       "        [-0.0812],\n",
       "        [-0.0797],\n",
       "        [-0.0800],\n",
       "        [-0.0800],\n",
       "        [-0.0797],\n",
       "        [-0.0812],\n",
       "        [-0.0938],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octane = Molecule.from_smiles(\"CCCCCCCC\")\n",
    "\n",
    "model.compute_property(octane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5831c8e",
   "metadata": {},
   "source": [
    "And we can compare that to the partial charges produced by the OpenFF Toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e12208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th>Magnitude</th><td style='text-align:left;'><pre>[-0.09216000225681525 -0.07999000039238197 -0.079319999481623<br> -0.07835999962228996 -0.07835999962228996 -0.079319999481623<br> -0.07999000039238197 -0.09207999792236549 0.03245000082712907<br> 0.03245000082712907 0.03245000082712907 0.03792999971371431<br> 0.03792999971371431 0.038880000893886275 0.038880000893886275<br> 0.03940999794464845 0.03940999794464845 0.03940999794464845<br> 0.03940999794464845 0.038880000893886275 0.038880000893886275<br> 0.03792999971371431 0.03792999971371431 0.03245000082712907<br> 0.03245000082712907 0.03245000082712907]</pre></td></tr><tr><th>Units</th><td style='text-align:left;'>elementary_charge</td></tr></tbody></table>"
      ],
      "text/latex": [
       "$\\begin{pmatrix}-0.09216000225681525 & -0.07999000039238197 & -0.079319999481623 & -0.07835999962228996 & -0.07835999962228996 & -0.079319999481623 & -0.07999000039238197 & -0.09207999792236549 & 0.03245000082712907 & 0.03245000082712907 & 0.03245000082712907 & 0.03792999971371431 & 0.03792999971371431 & 0.038880000893886275 & 0.038880000893886275 & 0.03940999794464845 & 0.03940999794464845 & 0.03940999794464845 & 0.03940999794464845 & 0.038880000893886275 & 0.038880000893886275 & 0.03792999971371431 & 0.03792999971371431 & 0.03245000082712907 & 0.03245000082712907 & 0.03245000082712907\\end{pmatrix}\\ \\mathrm{elementary\\_charge}$"
      ],
      "text/plain": [
       "array([-0.09216, -0.07999, -0.07932, -0.07836, -0.07836, -0.07932,\n",
       "       -0.07999, -0.09208,  0.03245,  0.03245,  0.03245,  0.03793,\n",
       "        0.03793,  0.03888,  0.03888,  0.03941,  0.03941,  0.03941,\n",
       "        0.03941,  0.03888,  0.03888,  0.03793,  0.03793,  0.03245,\n",
       "        0.03245,  0.03245]) <Unit('elementary_charge')>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "octane.assign_partial_charges(\"am1bcc\")\n",
    "octane.partial_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ef45d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tbody><tr><th>Magnitude</th><td style='text-align:left;'><pre>[0.0016340530262543562 0.0012301868544175032 0.0004043048964096907<br> 0.0015943414316727522 0.0015943190799309614 0.0004043048964096907<br> 0.0012301570520951155 0.0017140573607041243 0.0012287841393397378<br> 0.001228769238178544 0.0012287841393397378 0.0010045291139529275<br> 0.0010045291139529275 6.679158944349756e-05 6.677668828230371e-05<br> 0.00046320546131867896 0.0004632203624798728 0.00046320546131867896<br> 0.0004632203624798728 6.679158944349756e-05 6.677668828230371e-05<br> 0.0010045291139529275 0.0010045291139529275 0.001228769238178544<br> 0.0012287841393397378 0.001228769238178544]</pre></td></tr><tr><th>Units</th><td style='text-align:left;'>elementary_charge</td></tr></tbody></table>"
      ],
      "text/latex": [
       "$\\begin{pmatrix}0.0016340530262543562 & 0.0012301868544175032 & 0.0004043048964096907 & 0.0015943414316727522 & 0.0015943190799309614 & 0.0004043048964096907 & 0.0012301570520951155 & 0.0017140573607041243 & 0.0012287841393397378 & 0.001228769238178544 & 0.0012287841393397378 & 0.0010045291139529275 & 0.0010045291139529275 & 6.679158944349756\\times 10^{-5} & 6.677668828230371\\times 10^{-5} & 0.00046320546131867896 & 0.0004632203624798728 & 0.00046320546131867896 & 0.0004632203624798728 & 6.679158944349756\\times 10^{-5} & 6.677668828230371\\times 10^{-5} & 0.0010045291139529275 & 0.0010045291139529275 & 0.001228769238178544 & 0.0012287841393397378 & 0.001228769238178544\\end{pmatrix}\\ \\mathrm{elementary\\_charge}$"
      ],
      "text/plain": [
       "array([1.63405303e-03, 1.23018685e-03, 4.04304896e-04, 1.59434143e-03,\n",
       "       1.59431908e-03, 4.04304896e-04, 1.23015705e-03, 1.71405736e-03,\n",
       "       1.22878414e-03, 1.22876924e-03, 1.22878414e-03, 1.00452911e-03,\n",
       "       1.00452911e-03, 6.67915894e-05, 6.67766883e-05, 4.63205461e-04,\n",
       "       4.63220362e-04, 4.63205461e-04, 4.63220362e-04, 6.67915894e-05,\n",
       "       6.67766883e-05, 1.00452911e-03, 1.00452911e-03, 1.22876924e-03,\n",
       "       1.22878414e-03, 1.22876924e-03]) <Unit('elementary_charge')>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.compute_property(octane, as_numpy=True) * unit.elementary_charge\n",
    "np.abs(prediction - octane.partial_charges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a81b2f",
   "metadata": {},
   "source": [
    "All within 0.002 elementary charge units of true AM1BCC charges! Not too bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f055919",
   "metadata": {},
   "source": [
    "## Saving and loading our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6140",
   "metadata": {},
   "source": [
    "We can save the final model with the `model.save()` method. This'll let us store it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7020a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_alkane_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7dec08",
   "metadata": {},
   "source": [
    "When we want it again, we can use the `GNNModel.load()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6beb2faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0938],\n",
       "        [-0.0812],\n",
       "        [-0.0797],\n",
       "        [-0.0800],\n",
       "        [-0.0800],\n",
       "        [-0.0797],\n",
       "        [-0.0812],\n",
       "        [-0.0938],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0389],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337],\n",
       "        [ 0.0337]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_from_disk = GNNModel.load(\"trained_alkane_model.pt\")\n",
    "model_from_disk.compute_property(octane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24399a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
